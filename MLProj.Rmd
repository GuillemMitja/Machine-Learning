---
title: "Machine Learning Project"
author: "Guillem Mitj√†"
date: "28/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data preparation and analysis

We read the testing and training data
```{r}
testing <- read.csv("pml-testing.csv")
training <- read.csv("pml-training.csv")
```

The columns that are plenty of NA values are removed

```{r}
testing <- testing[,colSums(is.na(testing))==0]
training <- training[,colSums(is.na(training))==0]

```

Some other columns that are not used for the model are removed too

```{r}
training   <-training[,-c(1:7)]
testing <-testing[,-c(1:7)]

```


We load some packages

```{r}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
```

And the training data is split between two sets

```{r}
subSamples <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
subTraining <- training[subSamples, ] 
subTesting <- training[-subSamples, ]
```



Let's see how are the classes divided
```{r}
table(subTraining$classe)
```


# Using Machine Learning algorithm

```{r cache=TRUE}
decisionTreeMod <- train(classe ~., method='rpart', data=subTraining)

decisionTreePrediction <- predict(decisionTreeMod, subTesting)
confusionMatrix(subTesting$classe, decisionTreePrediction)
rpart.plot(decisionTreeMod$finalModel)
```



# Using Random forest

```{r cache=TRUE}
rfMod <- train(classe ~., method='rf', data=subTraining, ntree=128)
rfPrediction <- predict(rfMod, subTesting)
confusionMatrix(subTesting$classe, rfPrediction)
```

# Conclusions

We can see that the machine learning algorithm explais around the 50% of the classes. The Random forests explains almost the entire classes with the 99%.

